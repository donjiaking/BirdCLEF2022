{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3055f068",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:35:35.102377Z",
     "iopub.status.busy": "2022-03-29T06:35:35.100725Z",
     "iopub.status.idle": "2022-03-29T06:35:44.366843Z",
     "shell.execute_reply": "2022-03-29T06:35:44.366229Z"
    },
    "papermill": {
     "duration": 9.292738,
     "end_time": "2022-03-29T06:35:44.366999",
     "exception": false,
     "start_time": "2022-03-29T06:35:35.074261",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install ../input/torchlibrosa/torchlibrosa-0.0.5-py3-none-any.whl > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae516fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:35:44.412617Z",
     "iopub.status.busy": "2022-03-29T06:35:44.411774Z",
     "iopub.status.idle": "2022-03-29T06:35:54.703050Z",
     "shell.execute_reply": "2022-03-29T06:35:54.702486Z"
    },
    "papermill": {
     "duration": 10.314453,
     "end_time": "2022-03-29T06:35:54.703206",
     "exception": false,
     "start_time": "2022-03-29T06:35:44.388753",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install colorednoise > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e24250d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:35:54.745182Z",
     "iopub.status.busy": "2022-03-29T06:35:54.744471Z",
     "iopub.status.idle": "2022-03-29T06:35:54.746912Z",
     "shell.execute_reply": "2022-03-29T06:35:54.746470Z"
    },
    "papermill": {
     "duration": 0.024777,
     "end_time": "2022-03-29T06:35:54.747022",
     "exception": false,
     "start_time": "2022-03-29T06:35:54.722245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fold 0 test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5538c969",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:35:54.792617Z",
     "iopub.status.busy": "2022-03-29T06:35:54.791881Z",
     "iopub.status.idle": "2022-03-29T06:36:01.166648Z",
     "shell.execute_reply": "2022-03-29T06:36:01.165735Z"
    },
    "papermill": {
     "duration": 6.40051,
     "end_time": "2022-03-29T06:36:01.166846",
     "exception": false,
     "start_time": "2022-03-29T06:35:54.766336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import audioread\n",
    "import logging\n",
    "import gc\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import librosa\n",
    "import colorednoise as cn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as torchdata\n",
    "\n",
    "from contextlib import contextmanager\n",
    "from joblib import Parallel, delayed\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "from torchlibrosa.stft import LogmelFilterBank, Spectrogram\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "import albumentations.pytorch.transforms as T\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7dc58b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:01.214675Z",
     "iopub.status.busy": "2022-03-29T06:36:01.212751Z",
     "iopub.status.idle": "2022-03-29T06:36:01.283363Z",
     "shell.execute_reply": "2022-03-29T06:36:01.282858Z"
    },
    "papermill": {
     "duration": 0.09523,
     "end_time": "2022-03-29T06:36:01.283486",
     "exception": false,
     "start_time": "2022-03-29T06:36:01.188256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a99dcd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:01.330306Z",
     "iopub.status.busy": "2022-03-29T06:36:01.329752Z",
     "iopub.status.idle": "2022-03-29T06:36:06.779836Z",
     "shell.execute_reply": "2022-03-29T06:36:06.780256Z"
    },
    "papermill": {
     "duration": 5.475955,
     "end_time": "2022-03-29T06:36:06.780405",
     "exception": false,
     "start_time": "2022-03-29T06:36:01.304450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14852"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "all_path = glob.glob('../input/birdclef2022-audio-to-numpy-1-4/train_np/*/*.npy')\\\n",
    "+ glob.glob('../input/birdclef2022-audio-to-numpy-2-4/train_np/*/*.npy')\\\n",
    "+ glob.glob('../input/birdclef2022-audio-to-numpy-3-4/train_np/*/*.npy')\\\n",
    "+ glob.glob('../input/birdclef2022-audio-to-numpy-4-4/train_np/*/*.npy')\n",
    "\n",
    "len(all_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "898a9a2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:06.826597Z",
     "iopub.status.busy": "2022-03-29T06:36:06.826093Z",
     "iopub.status.idle": "2022-03-29T06:36:07.039149Z",
     "shell.execute_reply": "2022-03-29T06:36:07.039524Z"
    },
    "papermill": {
     "duration": 0.239133,
     "end_time": "2022-03-29T06:36:07.039686",
     "exception": false,
     "start_time": "2022-03-29T06:36:06.800553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>new_target</th>\n",
       "      <th>len_new_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-1.4930</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>08:00</td>\n",
       "      <td>https://www.xeno-canto.org/125458</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "      <td>afrsil1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['houspa', 'redava', 'zebdov']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>19.8801</td>\n",
       "      <td>-155.7254</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Dan Lane</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/175522</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "      <td>afrsil1 houspa redava zebdov</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>16.2901</td>\n",
       "      <td>-16.0321</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>https://www.xeno-canto.org/177993</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "      <td>afrsil1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>17.0922</td>\n",
       "      <td>54.2958</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Oscar Campbell</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:00</td>\n",
       "      <td>https://www.xeno-canto.org/205893</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "      <td>afrsil1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>21.4581</td>\n",
       "      <td>-157.7252</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Ross Gallardy</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16:30</td>\n",
       "      <td>https://www.xeno-canto.org/207431</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "      <td>afrsil1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                secondary_labels                     type  \\\n",
       "0       afrsil1                              []  ['call', 'flight call']   \n",
       "1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n",
       "2       afrsil1                              []         ['call', 'song']   \n",
       "3       afrsil1                              []   ['alarm call', 'call']   \n",
       "4       afrsil1                              []          ['flight call']   \n",
       "\n",
       "   latitude  longitude  scientific_name         common_name          author  \\\n",
       "0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n",
       "1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n",
       "2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n",
       "3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n",
       "4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n",
       "\n",
       "                                 url              filename  \\\n",
       "0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg   \n",
       "1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg   \n",
       "2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg   \n",
       "3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg   \n",
       "4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg   \n",
       "\n",
       "                     new_target  len_new_target  \n",
       "0                      afrsil1                1  \n",
       "1  afrsil1 houspa redava zebdov               4  \n",
       "2                      afrsil1                1  \n",
       "3                      afrsil1                1  \n",
       "4                      afrsil1                1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "\n",
    "train = pd.read_csv('../input/birdclef-2022/train_metadata.csv')\n",
    "\n",
    "\n",
    "train['new_target'] = train['primary_label'] + ' ' + train['secondary_labels'].map(lambda x: ' '.join(ast.literal_eval(x)))\n",
    "train['len_new_target'] = train['new_target'].map(lambda x: len(x.split()))\n",
    "# train['len_new_target'].value_counts()\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4b18920",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.109108Z",
     "iopub.status.busy": "2022-03-29T06:36:07.102444Z",
     "iopub.status.idle": "2022-03-29T06:36:07.113938Z",
     "shell.execute_reply": "2022-03-29T06:36:07.113379Z"
    },
    "papermill": {
     "duration": 0.053674,
     "end_time": "2022-03-29T06:36:07.114068",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.060394",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_path</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "      <td>bongul/XC473710.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "      <td>bongul/XC525248.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "      <td>bongul/XC325322.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "      <td>bongul/XC56898.ogg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "      <td>bongul/XC163068.ogg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_path             filename\n",
       "0  ../input/birdclef2022-audio-to-numpy-1-4/train...  bongul/XC473710.ogg\n",
       "1  ../input/birdclef2022-audio-to-numpy-1-4/train...  bongul/XC525248.ogg\n",
       "2  ../input/birdclef2022-audio-to-numpy-1-4/train...  bongul/XC325322.ogg\n",
       "3  ../input/birdclef2022-audio-to-numpy-1-4/train...   bongul/XC56898.ogg\n",
       "4  ../input/birdclef2022-audio-to-numpy-1-4/train...  bongul/XC163068.ogg"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_df = pd.DataFrame(all_path, columns=['file_path'])\n",
    "path_df['filename'] = path_df['file_path'].map(lambda x: x.split('/')[-2]+'/'+x.split('/')[-1][:-4])\n",
    "path_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "902f0a97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.162560Z",
     "iopub.status.busy": "2022-03-29T06:36:07.161630Z",
     "iopub.status.idle": "2022-03-29T06:36:07.207151Z",
     "shell.execute_reply": "2022-03-29T06:36:07.206628Z"
    },
    "papermill": {
     "duration": 0.071976,
     "end_time": "2022-03-29T06:36:07.207266",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.135290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14852, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>primary_label</th>\n",
       "      <th>secondary_labels</th>\n",
       "      <th>type</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>common_name</th>\n",
       "      <th>author</th>\n",
       "      <th>license</th>\n",
       "      <th>rating</th>\n",
       "      <th>time</th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>new_target</th>\n",
       "      <th>len_new_target</th>\n",
       "      <th>file_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'flight call']</td>\n",
       "      <td>12.3910</td>\n",
       "      <td>-1.4930</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>08:00</td>\n",
       "      <td>https://www.xeno-canto.org/125458</td>\n",
       "      <td>afrsil1/XC125458.ogg</td>\n",
       "      <td>afrsil1</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>['houspa', 'redava', 'zebdov']</td>\n",
       "      <td>['call']</td>\n",
       "      <td>19.8801</td>\n",
       "      <td>-155.7254</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Dan Lane</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>08:30</td>\n",
       "      <td>https://www.xeno-canto.org/175522</td>\n",
       "      <td>afrsil1/XC175522.ogg</td>\n",
       "      <td>afrsil1 houspa redava zebdov</td>\n",
       "      <td>4</td>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['call', 'song']</td>\n",
       "      <td>16.2901</td>\n",
       "      <td>-16.0321</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Bram Piot</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:30</td>\n",
       "      <td>https://www.xeno-canto.org/177993</td>\n",
       "      <td>afrsil1/XC177993.ogg</td>\n",
       "      <td>afrsil1</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['alarm call', 'call']</td>\n",
       "      <td>17.0922</td>\n",
       "      <td>54.2958</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Oscar Campbell</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11:00</td>\n",
       "      <td>https://www.xeno-canto.org/205893</td>\n",
       "      <td>afrsil1/XC205893.ogg</td>\n",
       "      <td>afrsil1</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>afrsil1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['flight call']</td>\n",
       "      <td>21.4581</td>\n",
       "      <td>-157.7252</td>\n",
       "      <td>Euodice cantans</td>\n",
       "      <td>African Silverbill</td>\n",
       "      <td>Ross Gallardy</td>\n",
       "      <td>Creative Commons Attribution-NonCommercial-Sha...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>16:30</td>\n",
       "      <td>https://www.xeno-canto.org/207431</td>\n",
       "      <td>afrsil1/XC207431.ogg</td>\n",
       "      <td>afrsil1</td>\n",
       "      <td>1</td>\n",
       "      <td>../input/birdclef2022-audio-to-numpy-1-4/train...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  primary_label                secondary_labels                     type  \\\n",
       "0       afrsil1                              []  ['call', 'flight call']   \n",
       "1       afrsil1  ['houspa', 'redava', 'zebdov']                 ['call']   \n",
       "2       afrsil1                              []         ['call', 'song']   \n",
       "3       afrsil1                              []   ['alarm call', 'call']   \n",
       "4       afrsil1                              []          ['flight call']   \n",
       "\n",
       "   latitude  longitude  scientific_name         common_name          author  \\\n",
       "0   12.3910    -1.4930  Euodice cantans  African Silverbill       Bram Piot   \n",
       "1   19.8801  -155.7254  Euodice cantans  African Silverbill        Dan Lane   \n",
       "2   16.2901   -16.0321  Euodice cantans  African Silverbill       Bram Piot   \n",
       "3   17.0922    54.2958  Euodice cantans  African Silverbill  Oscar Campbell   \n",
       "4   21.4581  -157.7252  Euodice cantans  African Silverbill   Ross Gallardy   \n",
       "\n",
       "                                             license  rating   time  \\\n",
       "0  Creative Commons Attribution-NonCommercial-Sha...     2.5  08:00   \n",
       "1  Creative Commons Attribution-NonCommercial-Sha...     3.5  08:30   \n",
       "2  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:30   \n",
       "3  Creative Commons Attribution-NonCommercial-Sha...     4.0  11:00   \n",
       "4  Creative Commons Attribution-NonCommercial-Sha...     3.0  16:30   \n",
       "\n",
       "                                 url              filename  \\\n",
       "0  https://www.xeno-canto.org/125458  afrsil1/XC125458.ogg   \n",
       "1  https://www.xeno-canto.org/175522  afrsil1/XC175522.ogg   \n",
       "2  https://www.xeno-canto.org/177993  afrsil1/XC177993.ogg   \n",
       "3  https://www.xeno-canto.org/205893  afrsil1/XC205893.ogg   \n",
       "4  https://www.xeno-canto.org/207431  afrsil1/XC207431.ogg   \n",
       "\n",
       "                     new_target  len_new_target  \\\n",
       "0                      afrsil1                1   \n",
       "1  afrsil1 houspa redava zebdov               4   \n",
       "2                      afrsil1                1   \n",
       "3                      afrsil1                1   \n",
       "4                      afrsil1                1   \n",
       "\n",
       "                                           file_path  \n",
       "0  ../input/birdclef2022-audio-to-numpy-1-4/train...  \n",
       "1  ../input/birdclef2022-audio-to-numpy-1-4/train...  \n",
       "2  ../input/birdclef2022-audio-to-numpy-1-4/train...  \n",
       "3  ../input/birdclef2022-audio-to-numpy-1-4/train...  \n",
       "4  ../input/birdclef2022-audio-to-numpy-1-4/train...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.merge(train, path_df, on='filename')\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "121f7b77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.257565Z",
     "iopub.status.busy": "2022-03-29T06:36:07.256487Z",
     "iopub.status.idle": "2022-03-29T06:36:07.287749Z",
     "shell.execute_reply": "2022-03-29T06:36:07.287142Z"
    },
    "papermill": {
     "duration": 0.057958,
     "end_time": "2022-03-29T06:36:07.287900",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.229942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for n, (trn_index, val_index) in enumerate(Fold.split(train, train['primary_label'])):\n",
    "    train.loc[val_index, 'kfold'] = int(n)\n",
    "train['kfold'] = train['kfold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa5336b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.339728Z",
     "iopub.status.busy": "2022-03-29T06:36:07.339059Z",
     "iopub.status.idle": "2022-03-29T06:36:07.569191Z",
     "shell.execute_reply": "2022-03-29T06:36:07.568642Z"
    },
    "papermill": {
     "duration": 0.25776,
     "end_time": "2022-03-29T06:36:07.569313",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.311553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train.to_csv('train_folds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0961270",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.625224Z",
     "iopub.status.busy": "2022-03-29T06:36:07.624452Z",
     "iopub.status.idle": "2022-03-29T06:36:07.626853Z",
     "shell.execute_reply": "2022-03-29T06:36:07.626302Z"
    },
    "papermill": {
     "duration": 0.034627,
     "end_time": "2022-03-29T06:36:07.626963",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.592336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    ######################\n",
    "    # Globals #\n",
    "    ######################\n",
    "    EXP_ID = 'EX005'\n",
    "    seed = 71\n",
    "    epochs = 23\n",
    "    cutmix_and_mixup_epochs = 18\n",
    "    folds = [0] # [0, 1, 2, 3, 4]\n",
    "    N_FOLDS = 5\n",
    "    LR = 1e-3\n",
    "    ETA_MIN = 1e-6\n",
    "    WEIGHT_DECAY = 1e-6\n",
    "    train_bs = 16 # 32\n",
    "    valid_bs = 32 # 64\n",
    "    base_model_name = \"tf_efficientnet_b0_ns\"\n",
    "    EARLY_STOPPING = True\n",
    "    DEBUG = False # True\n",
    "    EVALUATION = 'AUC'\n",
    "    apex = True\n",
    "\n",
    "    pooling = \"max\"\n",
    "    pretrained = True\n",
    "    num_classes = 152\n",
    "    in_channels = 3\n",
    "    target_columns = 'afrsil1 akekee akepa1 akiapo akikik amewig aniani apapan arcter \\\n",
    "                      barpet bcnher belkin1 bkbplo bknsti bkwpet blkfra blknod bongul \\\n",
    "                      brant brnboo brnnod brnowl brtcur bubsan buffle bulpet burpar buwtea \\\n",
    "                      cacgoo1 calqua cangoo canvas caster1 categr chbsan chemun chukar cintea \\\n",
    "                      comgal1 commyn compea comsan comwax coopet crehon dunlin elepai ercfra eurwig \\\n",
    "                      fragul gadwal gamqua glwgul gnwtea golphe grbher3 grefri gresca gryfra gwfgoo \\\n",
    "                      hawama hawcoo hawcre hawgoo hawhaw hawpet1 hoomer houfin houspa hudgod iiwi incter1 \\\n",
    "                      jabwar japqua kalphe kauama laugul layalb lcspet leasan leater1 lessca lesyel lobdow lotjae \\\n",
    "                      madpet magpet1 mallar3 masboo mauala maupar merlin mitpar moudov norcar norhar2 normoc norpin \\\n",
    "                      norsho nutman oahama omao osprey pagplo palila parjae pecsan peflov perfal pibgre pomjae puaioh \\\n",
    "                      reccar redava redjun redpha1 refboo rempar rettro ribgul rinduc rinphe rocpig rorpar rudtur ruff \\\n",
    "                      saffin sander semplo sheowl shtsan skylar snogoo sooshe sooter1 sopsku1 sora spodov sposan \\\n",
    "                      towsol wantat1 warwhe1 wesmea wessan wetshe whfibi whiter whttro wiltur yebcar yefcan zebdov'.split()\n",
    "\n",
    "    img_size = 224 # 128\n",
    "    main_metric = \"epoch_f1_at_03\"\n",
    "\n",
    "    period = 5\n",
    "    n_mels = 224 # 128\n",
    "    fmin = 20\n",
    "    fmax = 16000\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "    sample_rate = 32000\n",
    "    melspectrogram_parameters = {\n",
    "        \"n_mels\": 224, # 128,\n",
    "        \"fmin\": 20,\n",
    "        \"fmax\": 16000\n",
    "    }\n",
    "    \n",
    "    \n",
    "class AudioParams:\n",
    "    \"\"\"\n",
    "    Parameters used for the audio data\n",
    "    \"\"\"\n",
    "    sr = CFG.sample_rate\n",
    "    duration = CFG.period\n",
    "    # Melspectrogram\n",
    "    n_mels = CFG.n_mels\n",
    "    fmin = CFG.fmin\n",
    "    fmax = CFG.fmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9269c660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.707829Z",
     "iopub.status.busy": "2022-03-29T06:36:07.701415Z",
     "iopub.status.idle": "2022-03-29T06:36:07.710163Z",
     "shell.execute_reply": "2022-03-29T06:36:07.709760Z"
    },
    "papermill": {
     "duration": 0.060195,
     "end_time": "2022-03-29T06:36:07.710265",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.650070",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms: list):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, y: np.ndarray, sr):\n",
    "        for trns in self.transforms:\n",
    "            y = trns(y, sr)\n",
    "        return y\n",
    "\n",
    "\n",
    "class AudioTransform:\n",
    "    def __init__(self, always_apply=False, p=0.5):\n",
    "        self.always_apply = always_apply\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, y: np.ndarray, sr):\n",
    "        if self.always_apply:\n",
    "            return self.apply(y, sr=sr)\n",
    "        else:\n",
    "            if np.random.rand() < self.p:\n",
    "                return self.apply(y, sr=sr)\n",
    "            else:\n",
    "                return y\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class OneOf(Compose):\n",
    "    # https://github.com/albumentations-team/albumentations/blob/master/albumentations/core/composition.py\n",
    "    def __init__(self, transforms, p=0.5):\n",
    "        super().__init__(transforms)\n",
    "        self.p = p\n",
    "        transforms_ps = [t.p for t in transforms]\n",
    "        s = sum(transforms_ps)\n",
    "        self.transforms_ps = [t / s for t in transforms_ps]\n",
    "\n",
    "    def __call__(self, y: np.ndarray, sr):\n",
    "        data = y\n",
    "        if self.transforms_ps and (random.random() < self.p):\n",
    "            random_state = np.random.RandomState(random.randint(0, 2 ** 32 - 1))\n",
    "            t = random_state.choice(self.transforms, p=self.transforms_ps)\n",
    "            data = t(y, sr)\n",
    "        return data\n",
    "\n",
    "\n",
    "class Normalize(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=1):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        max_vol = np.abs(y).max()\n",
    "        y_vol = y * 1 / max_vol\n",
    "        return np.asfortranarray(y_vol)\n",
    "\n",
    "\n",
    "class NewNormalize(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=1):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        y_mm = y - y.mean()\n",
    "        return y_mm / y_mm.abs().max()\n",
    "\n",
    "\n",
    "class NoiseInjection(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_noise_level=0.5):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.noise_level = (0.0, max_noise_level)\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        noise_level = np.random.uniform(*self.noise_level)\n",
    "        noise = np.random.randn(len(y))\n",
    "        augmented = (y + noise * noise_level).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class GaussianNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y ** 2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        white_noise = np.random.randn(len(y))\n",
    "        a_white = np.sqrt(white_noise ** 2).max()\n",
    "        augmented = (y + white_noise * 1 / a_white * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PinkNoise(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, min_snr=5, max_snr=20):\n",
    "        super().__init__(always_apply, p)\n",
    "\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        snr = np.random.uniform(self.min_snr, self.max_snr)\n",
    "        a_signal = np.sqrt(y ** 2).max()\n",
    "        a_noise = a_signal / (10 ** (snr / 20))\n",
    "\n",
    "        pink_noise = cn.powerlaw_psd_gaussian(1, len(y))\n",
    "        a_pink = np.sqrt(pink_noise ** 2).max()\n",
    "        augmented = (y + pink_noise * 1 / a_pink * a_noise).astype(y.dtype)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class PitchShift(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_range=5):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.max_range = max_range\n",
    "\n",
    "    def apply(self, y: np.ndarray, sr, **params):\n",
    "        n_steps = np.random.randint(-self.max_range, self.max_range)\n",
    "        augmented = librosa.effects.pitch_shift(y, sr, n_steps)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "class TimeStretch(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, max_rate=1):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.max_rate = max_rate\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        rate = np.random.uniform(0, self.max_rate)\n",
    "        augmented = librosa.effects.time_stretch(y, rate)\n",
    "        return augmented\n",
    "\n",
    "\n",
    "def _db2float(db: float, amplitude=True):\n",
    "    if amplitude:\n",
    "        return 10 ** (db / 20)\n",
    "    else:\n",
    "        return 10 ** (db / 10)\n",
    "\n",
    "\n",
    "def volume_down(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for decreasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to decrease\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with decreased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(-db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "def volume_up(y: np.ndarray, db: float):\n",
    "    \"\"\"\n",
    "    Low level API for increasing the volume\n",
    "    Parameters\n",
    "    ----------\n",
    "    y: numpy.ndarray\n",
    "        stereo / monaural input audio\n",
    "    db: float\n",
    "        how much decibel to increase\n",
    "    Returns\n",
    "    -------\n",
    "    applied: numpy.ndarray\n",
    "        audio with increased volume\n",
    "    \"\"\"\n",
    "    applied = y * _db2float(db)\n",
    "    return applied\n",
    "\n",
    "\n",
    "class RandomVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        if db >= 0:\n",
    "            return volume_up(y, db)\n",
    "        else:\n",
    "            return volume_down(y, db)\n",
    "\n",
    "\n",
    "class CosineVolume(AudioTransform):\n",
    "    def __init__(self, always_apply=False, p=0.5, limit=10):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.limit = limit\n",
    "\n",
    "    def apply(self, y: np.ndarray, **params):\n",
    "        db = np.random.uniform(-self.limit, self.limit)\n",
    "        cosine = np.cos(np.arange(len(y)) / len(y) * np.pi * 2)\n",
    "        dbs = _db2float(cosine * db)\n",
    "        return y * dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b8ab38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.762746Z",
     "iopub.status.busy": "2022-03-29T06:36:07.762068Z",
     "iopub.status.idle": "2022-03-29T06:36:07.805363Z",
     "shell.execute_reply": "2022-03-29T06:36:07.805954Z"
    },
    "papermill": {
     "duration": 0.072771,
     "end_time": "2022-03-29T06:36:07.806118",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.733347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "OUTPUT_DIR = f'./'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "   \n",
    "    \n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "set_seed(CFG.seed)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3e05d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.870527Z",
     "iopub.status.busy": "2022-03-29T06:36:07.868921Z",
     "iopub.status.idle": "2022-03-29T06:36:07.871122Z",
     "shell.execute_reply": "2022-03-29T06:36:07.871520Z"
    },
    "papermill": {
     "duration": 0.042458,
     "end_time": "2022-03-29T06:36:07.871641",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.829183",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_loss(y_true, y_pred):\n",
    "    return metrics.roc_auc_score(np.array(y_true), np.array(y_pred))\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Training helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "\n",
    "class MetricMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.y_true = []\n",
    "        self.y_pred = []\n",
    "    \n",
    "    def update(self, y_true, y_pred):\n",
    "        self.y_true.extend(y_true.cpu().detach().numpy().tolist())\n",
    "        self.y_pred.extend(y_pred[\"clipwise_output\"].cpu().detach().numpy().tolist())\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        self.f1_03 = metrics.f1_score(np.array(self.y_true), np.array(self.y_pred) > 0.3, average=\"micro\")\n",
    "        self.f1_05 = metrics.f1_score(np.array(self.y_true), np.array(self.y_pred) > 0.5, average=\"micro\")\n",
    "        \n",
    "        return {\n",
    "            \"f1_at_03\" : self.f1_03,\n",
    "            \"f1_at_05\" : self.f1_05,\n",
    "        }\n",
    "    \n",
    "    \n",
    "# https://www.kaggle.com/c/rfcx-species-audio-detection/discussion/213075\n",
    "class BCEFocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        bce_loss = nn.BCEWithLogitsLoss(reduction='none')(preds, targets)\n",
    "        probas = torch.sigmoid(preds)\n",
    "        loss = targets * self.alpha * \\\n",
    "            (1. - probas)**self.gamma * bce_loss + \\\n",
    "            (1. - targets) * probas**self.gamma * bce_loss\n",
    "        loss = loss.mean()\n",
    "        return loss\n",
    "\n",
    "\n",
    "class BCEFocal2WayLoss(nn.Module):\n",
    "    def __init__(self, weights=[1, 1], class_weights=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.focal = BCEFocalLoss()\n",
    "\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input_ = input[\"logit\"]\n",
    "        target = target.float()\n",
    "\n",
    "        framewise_output = input[\"framewise_logit\"]\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "\n",
    "        loss = self.focal(input_, target)\n",
    "        aux_loss = self.focal(clipwise_output_with_max, target)\n",
    "\n",
    "        return self.weights[0] * loss + self.weights[1] * aux_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06852c6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:07.942280Z",
     "iopub.status.busy": "2022-03-29T06:36:07.941539Z",
     "iopub.status.idle": "2022-03-29T06:36:07.947732Z",
     "shell.execute_reply": "2022-03-29T06:36:07.948325Z"
    },
    "papermill": {
     "duration": 0.05329,
     "end_time": "2022-03-29T06:36:07.948507",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.895217",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:691: FutureWarning: This class has been deprecated. Please use CoarseDropout\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "def compute_melspec(y, params):\n",
    "    \"\"\"\n",
    "    Computes a mel-spectrogram and puts it at decibel scale\n",
    "    Arguments:\n",
    "        y {np array} -- signal\n",
    "        params {AudioParams} -- Parameters to use for the spectrogram. Expected to have the attributes sr, n_mels, f_min, f_max\n",
    "    Returns:\n",
    "        np array -- Mel-spectrogram\n",
    "    \"\"\"\n",
    "    melspec = librosa.feature.melspectrogram(\n",
    "        y=y, sr=params.sr, n_mels=params.n_mels, fmin=params.fmin, fmax=params.fmax,\n",
    "    )\n",
    "\n",
    "    melspec = librosa.power_to_db(melspec).astype(np.float32)\n",
    "    return melspec\n",
    "\n",
    "\n",
    "def crop_or_pad(y, length, sr, train=True, probs=None):\n",
    "    \"\"\"\n",
    "    Crops an array to a chosen length\n",
    "    Arguments:\n",
    "        y {1D np array} -- Array to crop\n",
    "        length {int} -- Length of the crop\n",
    "        sr {int} -- Sampling rate\n",
    "    Keyword Arguments:\n",
    "        train {bool} -- Whether we are at train time. If so, crop randomly, else return the beginning of y (default: {True})\n",
    "        probs {None or numpy array} -- Probabilities to use to chose where to crop (default: {None})\n",
    "    Returns:\n",
    "        1D np array -- Cropped array\n",
    "    \"\"\"\n",
    "    if len(y) <= length:\n",
    "        y = np.concatenate([y, np.zeros(length - len(y))])\n",
    "    else:\n",
    "        if not train:\n",
    "            start = 0\n",
    "        elif probs is None:\n",
    "            start = np.random.randint(len(y) - length)\n",
    "        else:\n",
    "            start = (\n",
    "                    np.random.choice(np.arange(len(probs)), p=probs) + np.random.random()\n",
    "            )\n",
    "            start = int(sr * (start))\n",
    "\n",
    "        y = y[start: start + length]\n",
    "\n",
    "    return y.astype(np.float32)\n",
    "\n",
    "\n",
    "def mono_to_color(X, eps=1e-6, mean=None, std=None):\n",
    "    \"\"\"\n",
    "    Converts a one channel array to a 3 channel one in [0, 255]\n",
    "    Arguments:\n",
    "        X {numpy array [H x W]} -- 2D array to convert\n",
    "    Keyword Arguments:\n",
    "        eps {float} -- To avoid dividing by 0 (default: {1e-6})\n",
    "        mean {None or np array} -- Mean for normalization (default: {None})\n",
    "        std {None or np array} -- Std for normalization (default: {None})\n",
    "    Returns:\n",
    "        numpy array [3 x H x W] -- RGB numpy array\n",
    "    \"\"\"\n",
    "    X = np.stack([X, X, X], axis=-1)\n",
    "\n",
    "    # Standardize\n",
    "    mean = mean or X.mean()\n",
    "    std = std or X.std()\n",
    "    X = (X - mean) / (std + eps)\n",
    "\n",
    "    # Normalize to [0, 255]\n",
    "    _min, _max = X.min(), X.max()\n",
    "\n",
    "    if (_max - _min) > eps:\n",
    "        V = np.clip(X, _min, _max)\n",
    "        V = 255 * (V - _min) / (_max - _min)\n",
    "        V = V.astype(np.uint8)\n",
    "    else:\n",
    "        V = np.zeros_like(X, dtype=np.uint8)\n",
    "\n",
    "    return V\n",
    "\n",
    "\n",
    "mean = (0.485, 0.456, 0.406) # RGB\n",
    "std = (0.229, 0.224, 0.225) # RGB\n",
    "\n",
    "albu_transforms = {\n",
    "    'train' : A.Compose([\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.OneOf([\n",
    "                A.Cutout(max_h_size=5, max_w_size=16),\n",
    "                A.CoarseDropout(max_holes=4),\n",
    "            ], p=0.5),\n",
    "            A.Normalize(mean, std),\n",
    "    ]),\n",
    "    'valid' : A.Compose([\n",
    "            A.Normalize(mean, std),\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "class WaveformDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 mode='train'):\n",
    "        self.df = df\n",
    "        self.mode = mode\n",
    "\n",
    "        if mode == 'train':\n",
    "            self.wave_transforms = Compose(\n",
    "                [\n",
    "                    OneOf(\n",
    "                        [\n",
    "                            NoiseInjection(p=1, max_noise_level=0.04),\n",
    "                            GaussianNoise(p=1, min_snr=5, max_snr=20),\n",
    "                            PinkNoise(p=1, min_snr=5, max_snr=20),\n",
    "                        ],\n",
    "                        p=0.2,\n",
    "                    ),\n",
    "                    RandomVolume(p=0.2, limit=4),\n",
    "                    Normalize(p=1),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            self.wave_transforms = Compose(\n",
    "                [\n",
    "                    Normalize(p=1),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        SR = 32000\n",
    "        sample = self.df.loc[idx, :]\n",
    "        \n",
    "        wav_path = sample[\"file_path\"]\n",
    "        labels = sample[\"new_target\"]\n",
    "\n",
    "        y = np.load(wav_path)\n",
    "\n",
    "        # SEC = int(len(y)/2/SR)\n",
    "        # if SEC > 0:\n",
    "        #     start = np.random.randint(SEC)\n",
    "        #     end = start+AudioParams.duration\n",
    "        if len(y) > 0:\n",
    "            y = y[:AudioParams.duration*SR]\n",
    "\n",
    "            if self.wave_transforms:\n",
    "                y = self.wave_transforms(y, sr=SR)\n",
    "\n",
    "        y = np.concatenate([y, y, y])[:AudioParams.duration * AudioParams.sr] \n",
    "        y = crop_or_pad(y, AudioParams.duration * AudioParams.sr, sr=AudioParams.sr, train=True, probs=None)\n",
    "        image = compute_melspec(y, AudioParams)\n",
    "        image = mono_to_color(image)\n",
    "        image = image.astype(np.uint8)\n",
    "        \n",
    "        # image = np.load(wav_path) # (224, 313, 3)\n",
    "        image = albu_transforms[self.mode](image=image)['image']\n",
    "        image = image.T\n",
    "        \n",
    "        targets = np.zeros(len(CFG.target_columns), dtype=float)\n",
    "        for ebird_code in labels.split():\n",
    "            targets[CFG.target_columns.index(ebird_code)] = 1.0\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"targets\": targets,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "078ab200",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:08.028883Z",
     "iopub.status.busy": "2022-03-29T06:36:08.023471Z",
     "iopub.status.idle": "2022-03-29T06:36:08.032280Z",
     "shell.execute_reply": "2022-03-29T06:36:08.032678Z"
    },
    "papermill": {
     "duration": 0.059003,
     "end_time": "2022-03-29T06:36:08.032827",
     "exception": false,
     "start_time": "2022-03-29T06:36:07.973824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n",
    "    \"\"\"Pad framewise_output to the same length as input frames. The pad value\n",
    "    is the same as the value of the last frame.\n",
    "    Args:\n",
    "      framewise_output: (batch_size, frames_num, classes_num)\n",
    "      frames_num: int, number of frames to pad\n",
    "    Outputs:\n",
    "      output: (batch_size, frames_num, classes_num)\n",
    "    \"\"\"\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.spec_augmenter = SpecAugmentation(time_drop_width=64//2, time_stripes_num=2,\n",
    "                                               freq_drop_width=8//2, freq_stripes_num=2)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(CFG.n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        if hasattr(base_model, \"fc\"):\n",
    "            in_features = base_model.fc.in_features\n",
    "        else:\n",
    "            in_features = base_model.classifier.in_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data # (batch_size, 3, time_steps, mel_bins)\n",
    "\n",
    "        frames_num = x.shape[2]\n",
    "\n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        if self.training:\n",
    "            if random.random() < 0.25:\n",
    "                x = self.spec_augmenter(x)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Aggregate in frequency axis\n",
    "        x = torch.mean(x, dim=3)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        interpolate_ratio = frames_num // segmentwise_output.size(1)\n",
    "\n",
    "        # Get framewise output\n",
    "        framewise_output = interpolate(segmentwise_output,\n",
    "                                       interpolate_ratio)\n",
    "        framewise_output = pad_framewise_output(framewise_output, frames_num)\n",
    "\n",
    "        framewise_logit = interpolate(segmentwise_logit, interpolate_ratio)\n",
    "        framewise_logit = pad_framewise_output(framewise_logit, frames_num)\n",
    "\n",
    "        output_dict = {\n",
    "            'framewise_output': framewise_output,\n",
    "            'clipwise_output': clipwise_output,\n",
    "            'logit': logit,\n",
    "            'framewise_logit': framewise_logit,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9f870c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:08.088992Z",
     "iopub.status.busy": "2022-03-29T06:36:08.088208Z",
     "iopub.status.idle": "2022-03-29T06:36:08.097338Z",
     "shell.execute_reply": "2022-03-29T06:36:08.096948Z"
    },
    "papermill": {
     "duration": 0.040177,
     "end_time": "2022-03-29T06:36:08.097439",
     "exception": false,
     "start_time": "2022-03-29T06:36:08.057262",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "\n",
    "def cutmix(data, targets, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    data[:, :, bbx1:bbx2, bby1:bby2] = data[indices, :, bbx1:bbx2, bby1:bby2]\n",
    "    # adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
    "\n",
    "    new_targets = [targets, shuffled_targets, lam]\n",
    "    return data, new_targets\n",
    "\n",
    "def mixup(data, targets, alpha):\n",
    "    indices = torch.randperm(data.size(0))\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_targets = targets[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    new_data = data * lam + shuffled_data * (1 - lam)\n",
    "    new_targets = [targets, shuffled_targets, lam]\n",
    "    return new_data, new_targets\n",
    "\n",
    "\n",
    "def cutmix_criterion(preds, new_targets):\n",
    "    targets1, targets2, lam = new_targets[0], new_targets[1], new_targets[2]\n",
    "    criterion = BCEFocal2WayLoss()\n",
    "    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n",
    "\n",
    "def mixup_criterion(preds, new_targets):\n",
    "    targets1, targets2, lam = new_targets[0], new_targets[1], new_targets[2]\n",
    "    criterion = BCEFocal2WayLoss()\n",
    "    return lam * criterion(preds, targets1) + (1 - lam) * criterion(preds, targets2)\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    loss_fct = BCEFocal2WayLoss()\n",
    "    loss = loss_fct(logits, targets)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d88385a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:08.162018Z",
     "iopub.status.busy": "2022-03-29T06:36:08.161278Z",
     "iopub.status.idle": "2022-03-29T06:36:08.163239Z",
     "shell.execute_reply": "2022-03-29T06:36:08.163606Z"
    },
    "papermill": {
     "duration": 0.042457,
     "end_time": "2022-03-29T06:36:08.163738",
     "exception": false,
     "start_time": "2022-03-29T06:36:08.121281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, device, optimizer, scheduler):\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    scores = MetricMeter()\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    \n",
    "    for data in tk0:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = data['image'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "        with autocast(enabled=CFG.apex):\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        scores.update(targets, outputs)\n",
    "        tk0.set_postfix(loss=losses.avg)\n",
    "    return scores.avg, losses.avg\n",
    "\n",
    "\n",
    "def train_mixup_cutmix_fn(model, data_loader, device, optimizer, scheduler):\n",
    "    model.train()\n",
    "    scaler = GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    scores = MetricMeter()\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "\n",
    "    for data in tk0:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = data['image'].to(device)\n",
    "        targets = data['targets'].to(device)\n",
    "\n",
    "        if np.random.rand()<0.5:\n",
    "            inputs, new_targets = mixup(inputs, targets, 0.4)\n",
    "            with autocast(enabled=CFG.apex):\n",
    "                outputs = model(inputs)\n",
    "                loss = mixup_criterion(outputs, new_targets) \n",
    "        else:\n",
    "            inputs, new_targets = cutmix(inputs, targets, 0.4)\n",
    "            with autocast(enabled=CFG.apex):\n",
    "                outputs = model(inputs)\n",
    "                loss = cutmix_criterion(outputs, new_targets)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        scheduler.step()\n",
    "        losses.update(loss.item(), inputs.size(0))\n",
    "        scores.update(new_targets[0], outputs)\n",
    "        tk0.set_postfix(loss=losses.avg)\n",
    "    return scores.avg, losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(model, data_loader, device):\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    scores = MetricMeter()\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    valid_preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in tk0:\n",
    "            inputs = data['image'].to(device)\n",
    "            targets = data['targets'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            scores.update(targets, outputs)\n",
    "            tk0.set_postfix(loss=losses.avg)\n",
    "    return scores.avg, losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ac45eb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:08.220013Z",
     "iopub.status.busy": "2022-03-29T06:36:08.219240Z",
     "iopub.status.idle": "2022-03-29T06:36:08.226843Z",
     "shell.execute_reply": "2022-03-29T06:36:08.226388Z"
    },
    "papermill": {
     "duration": 0.039359,
     "end_time": "2022-03-29T06:36:08.226946",
     "exception": false,
     "start_time": "2022-03-29T06:36:08.187587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference_fn(model, data_loader, device):\n",
    "    model.eval()\n",
    "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
    "    final_output = []\n",
    "    final_target = []\n",
    "    with torch.no_grad():\n",
    "        for b_idx, data in enumerate(tk0):\n",
    "            inputs = data['image'].to(device)\n",
    "            targets = data['targets'].to(device).detach().cpu().numpy().tolist()\n",
    "            output = model(inputs)\n",
    "            output = output[\"clipwise_output\"].cpu().detach().cpu().numpy().tolist()\n",
    "            final_output.extend(output)\n",
    "            final_target.extend(targets)\n",
    "    return final_output, final_target\n",
    "\n",
    "\n",
    "def calc_cv(model_paths):\n",
    "    df = pd.read_csv('train_folds.csv')\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for fold, model_path in enumerate(model_paths):\n",
    "        model = TimmSED(\n",
    "            base_model_name=CFG.base_model_name,\n",
    "            pretrained=CFG.pretrained,\n",
    "            num_classes=CFG.num_classes,\n",
    "            in_channels=CFG.in_channels)\n",
    "\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        model.eval()\n",
    "\n",
    "        val_df = df[df.kfold == fold].reset_index(drop=True)\n",
    "        dataset = WaveformDataset(df=val_df, mode='valid')\n",
    "        dataloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n",
    "        )\n",
    "\n",
    "        final_output, final_target = inference_fn(model, dataloader, device)\n",
    "        y_pred.extend(final_output)\n",
    "        y_true.extend(final_target)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        f1_03 = metrics.f1_score(np.array(y_true), np.array(y_pred) > 0.3, average=\"micro\")\n",
    "        print(f'micro f1_0.3 {f1_03}')\n",
    "\n",
    "    f1_03 = metrics.f1_score(np.array(y_true), np.array(y_pred) > 0.3, average=\"micro\")\n",
    "    f1_05 = metrics.f1_score(np.array(y_true), np.array(y_pred) > 0.5, average=\"micro\")\n",
    "\n",
    "    print(f'overall micro f1_0.3 {f1_03}')\n",
    "    print(f'overall micro f1_0.5 {f1_05}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16e0eb64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T06:36:08.288378Z",
     "iopub.status.busy": "2022-03-29T06:36:08.287598Z",
     "iopub.status.idle": "2022-03-29T15:05:22.423226Z",
     "shell.execute_reply": "2022-03-29T15:05:22.422789Z"
    },
    "papermill": {
     "duration": 30554.172366,
     "end_time": "2022-03-29T15:05:22.423355",
     "exception": false,
     "start_time": "2022-03-29T06:36:08.250989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Fold 0 Training\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/tf_efficientnet_b0_ns-c0e6a31c.pth\" to /root/.cache/torch/hub/checkpoints/tf_efficientnet_b0_ns-c0e6a31c.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 1 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [17:48<00:00,  1.44s/it, loss=0.0143]\n",
      "100%|| 93/93 [03:56<00:00,  2.55s/it, loss=0.00791]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - avg_train_loss: 0.01428  avg_val_loss: 0.00791  time: 1307s\n",
      "Epoch 1 - train_f1_at_03:0.00435  valid_f1_at_03:0.00181\n",
      "Epoch 1 - train_f1_at_05:0.00148  valid_f1_at_05:0.00000\n",
      ">>>>>>>> Model Improved From -inf ----> 0.0018077734257306419\n",
      "other scores here... 0.0018077734257306419, 0.0\n",
      "Starting 2 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/743 [00:00<?, ?it/s][W accumulate_grad.h:185] Warning: grad and param do not obey the gradient layout contract. This is not an error, but may impair performance.\n",
      "grad.sizes() = [1280, 320, 1, 1], strides() = [320, 1, 1, 1]\n",
      "param.sizes() = [1280, 320, 1, 1], strides() = [320, 1, 320, 320] (function operator())\n",
      "100%|| 743/743 [17:44<00:00,  1.43s/it, loss=0.00842]\n",
      "100%|| 93/93 [04:01<00:00,  2.60s/it, loss=0.0067]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - avg_train_loss: 0.00842  avg_val_loss: 0.00670  time: 1308s\n",
      "Epoch 2 - train_f1_at_03:0.01570  valid_f1_at_03:0.21921\n",
      "Epoch 2 - train_f1_at_05:0.00030  valid_f1_at_05:0.04888\n",
      ">>>>>>>> Model Improved From 0.0018077734257306419 ----> 0.2192118226600985\n",
      "other scores here... 0.2192118226600985, 0.04888103651354535\n",
      "Starting 3 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [17:55<00:00,  1.45s/it, loss=0.00769]\n",
      "100%|| 93/93 [03:53<00:00,  2.51s/it, loss=0.00599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - avg_train_loss: 0.00769  avg_val_loss: 0.00599  time: 1312s\n",
      "Epoch 3 - train_f1_at_03:0.05516  valid_f1_at_03:0.34354\n",
      "Epoch 3 - train_f1_at_05:0.00419  valid_f1_at_05:0.13431\n",
      ">>>>>>>> Model Improved From 0.2192118226600985 ----> 0.34354485776805255\n",
      "other scores here... 0.34354485776805255, 0.1343073897162124\n",
      "Starting 4 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:04<00:00,  1.46s/it, loss=0.00705]\n",
      "100%|| 93/93 [04:01<00:00,  2.60s/it, loss=0.00617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - avg_train_loss: 0.00705  avg_val_loss: 0.00617  time: 1328s\n",
      "Epoch 4 - train_f1_at_03:0.11181  valid_f1_at_03:0.31617\n",
      "Epoch 4 - train_f1_at_05:0.01131  valid_f1_at_05:0.09635\n",
      "Starting 5 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:09<00:00,  1.47s/it, loss=0.00675]\n",
      "100%|| 93/93 [03:58<00:00,  2.57s/it, loss=0.00492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - avg_train_loss: 0.00675  avg_val_loss: 0.00492  time: 1330s\n",
      "Epoch 5 - train_f1_at_03:0.15842  valid_f1_at_03:0.47686\n",
      "Epoch 5 - train_f1_at_05:0.01978  valid_f1_at_05:0.26604\n",
      ">>>>>>>> Model Improved From 0.34354485776805255 ----> 0.47686401293190533\n",
      "other scores here... 0.47686401293190533, 0.26604312808521696\n",
      "Starting 6 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [17:49<00:00,  1.44s/it, loss=0.00661]\n",
      "100%|| 93/93 [03:56<00:00,  2.55s/it, loss=0.00458]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - avg_train_loss: 0.00661  avg_val_loss: 0.00458  time: 1308s\n",
      "Epoch 6 - train_f1_at_03:0.17574  valid_f1_at_03:0.52861\n",
      "Epoch 6 - train_f1_at_05:0.02541  valid_f1_at_05:0.31408\n",
      ">>>>>>>> Model Improved From 0.47686401293190533 ----> 0.5286135693215339\n",
      "other scores here... 0.5286135693215339, 0.3140830800405269\n",
      "Starting 7 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:10<00:00,  1.47s/it, loss=0.00636]\n",
      "100%|| 93/93 [04:00<00:00,  2.59s/it, loss=0.00486]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - avg_train_loss: 0.00636  avg_val_loss: 0.00486  time: 1332s\n",
      "Epoch 7 - train_f1_at_03:0.21835  valid_f1_at_03:0.47225\n",
      "Epoch 7 - train_f1_at_05:0.02990  valid_f1_at_05:0.19516\n",
      "Starting 8 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:22<00:00,  1.48s/it, loss=0.00604]\n",
      "100%|| 93/93 [04:05<00:00,  2.64s/it, loss=0.00494]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - avg_train_loss: 0.00604  avg_val_loss: 0.00494  time: 1349s\n",
      "Epoch 8 - train_f1_at_03:0.25825  valid_f1_at_03:0.47027\n",
      "Epoch 8 - train_f1_at_05:0.04798  valid_f1_at_05:0.24842\n",
      "Starting 9 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:31<00:00,  1.50s/it, loss=0.00598]\n",
      "100%|| 93/93 [04:16<00:00,  2.76s/it, loss=0.00405]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - avg_train_loss: 0.00598  avg_val_loss: 0.00405  time: 1370s\n",
      "Epoch 9 - train_f1_at_03:0.25486  valid_f1_at_03:0.59047\n",
      "Epoch 9 - train_f1_at_05:0.04966  valid_f1_at_05:0.41751\n",
      ">>>>>>>> Model Improved From 0.5286135693215339 ----> 0.5904692349826547\n",
      "other scores here... 0.5904692349826547, 0.4175147928994083\n",
      "Starting 10 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:38<00:00,  1.51s/it, loss=0.00594]\n",
      "100%|| 93/93 [04:01<00:00,  2.60s/it, loss=0.0039]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - avg_train_loss: 0.00594  avg_val_loss: 0.00390  time: 1362s\n",
      "Epoch 10 - train_f1_at_03:0.27095  valid_f1_at_03:0.61187\n",
      "Epoch 10 - train_f1_at_05:0.05935  valid_f1_at_05:0.40938\n",
      ">>>>>>>> Model Improved From 0.5904692349826547 ----> 0.6118687799483966\n",
      "other scores here... 0.6118687799483966, 0.409384725879818\n",
      "Starting 11 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:08<00:00,  1.47s/it, loss=0.00572]\n",
      "100%|| 93/93 [03:55<00:00,  2.54s/it, loss=0.00427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - avg_train_loss: 0.00572  avg_val_loss: 0.00427  time: 1326s\n",
      "Epoch 11 - train_f1_at_03:0.29936  valid_f1_at_03:0.57511\n",
      "Epoch 11 - train_f1_at_05:0.06873  valid_f1_at_05:0.35393\n",
      "Starting 12 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [17:55<00:00,  1.45s/it, loss=0.00548]\n",
      "100%|| 93/93 [03:57<00:00,  2.55s/it, loss=0.00438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - avg_train_loss: 0.00548  avg_val_loss: 0.00438  time: 1314s\n",
      "Epoch 12 - train_f1_at_03:0.33085  valid_f1_at_03:0.53540\n",
      "Epoch 12 - train_f1_at_05:0.07696  valid_f1_at_05:0.23175\n",
      "Starting 13 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [17:59<00:00,  1.45s/it, loss=0.00558]\n",
      "100%|| 93/93 [04:04<00:00,  2.63s/it, loss=0.00362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - avg_train_loss: 0.00558  avg_val_loss: 0.00362  time: 1325s\n",
      "Epoch 13 - train_f1_at_03:0.31759  valid_f1_at_03:0.64545\n",
      "Epoch 13 - train_f1_at_05:0.07382  valid_f1_at_05:0.44491\n",
      ">>>>>>>> Model Improved From 0.6118687799483966 ----> 0.6454529018260713\n",
      "other scores here... 0.6454529018260713, 0.4449112978524743\n",
      "Starting 14 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:09<00:00,  1.47s/it, loss=0.00548]\n",
      "100%|| 93/93 [04:01<00:00,  2.59s/it, loss=0.00362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - avg_train_loss: 0.00548  avg_val_loss: 0.00362  time: 1332s\n",
      "Epoch 14 - train_f1_at_03:0.33630  valid_f1_at_03:0.65418\n",
      "Epoch 14 - train_f1_at_05:0.09031  valid_f1_at_05:0.38574\n",
      ">>>>>>>> Model Improved From 0.6454529018260713 ----> 0.6541750045479352\n",
      "other scores here... 0.6541750045479352, 0.38573508005822416\n",
      "Starting 15 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:00<00:00,  1.45s/it, loss=0.0052]\n",
      "100%|| 93/93 [04:00<00:00,  2.59s/it, loss=0.00398]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - avg_train_loss: 0.00520  avg_val_loss: 0.00398  time: 1322s\n",
      "Epoch 15 - train_f1_at_03:0.35141  valid_f1_at_03:0.60803\n",
      "Epoch 15 - train_f1_at_05:0.10317  valid_f1_at_05:0.36528\n",
      "Starting 16 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:03<00:00,  1.46s/it, loss=0.00518]\n",
      "100%|| 93/93 [04:11<00:00,  2.70s/it, loss=0.00392]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - avg_train_loss: 0.00518  avg_val_loss: 0.00392  time: 1337s\n",
      "Epoch 16 - train_f1_at_03:0.37365  valid_f1_at_03:0.63115\n",
      "Epoch 16 - train_f1_at_05:0.09982  valid_f1_at_05:0.43114\n",
      "Starting 17 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:12<00:00,  1.47s/it, loss=0.00522]\n",
      "100%|| 93/93 [03:52<00:00,  2.50s/it, loss=0.00344]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - avg_train_loss: 0.00522  avg_val_loss: 0.00344  time: 1327s\n",
      "Epoch 17 - train_f1_at_03:0.34859  valid_f1_at_03:0.68226\n",
      "Epoch 17 - train_f1_at_05:0.10506  valid_f1_at_05:0.45357\n",
      ">>>>>>>> Model Improved From 0.6541750045479352 ----> 0.6822625698324022\n",
      "other scores here... 0.6822625698324022, 0.45357474466109565\n",
      "Starting 18 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:05<00:00,  1.46s/it, loss=0.00519]\n",
      "100%|| 93/93 [03:59<00:00,  2.57s/it, loss=0.00342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - avg_train_loss: 0.00519  avg_val_loss: 0.00342  time: 1326s\n",
      "Epoch 18 - train_f1_at_03:0.38947  valid_f1_at_03:0.68670\n",
      "Epoch 18 - train_f1_at_05:0.11155  valid_f1_at_05:0.46377\n",
      ">>>>>>>> Model Improved From 0.6822625698324022 ----> 0.6867027782631487\n",
      "other scores here... 0.6867027782631487, 0.46377480387632675\n",
      "Starting 19 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [17:54<00:00,  1.45s/it, loss=0.00339]\n",
      "100%|| 93/93 [04:06<00:00,  2.65s/it, loss=0.00404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - avg_train_loss: 0.00339  avg_val_loss: 0.00404  time: 1322s\n",
      "Epoch 19 - train_f1_at_03:0.68334  valid_f1_at_03:0.63491\n",
      "Epoch 19 - train_f1_at_05:0.48902  valid_f1_at_05:0.54480\n",
      "Starting 20 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:05<00:00,  1.46s/it, loss=0.00319]\n",
      "100%|| 93/93 [04:06<00:00,  2.65s/it, loss=0.00371]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - avg_train_loss: 0.00319  avg_val_loss: 0.00371  time: 1333s\n",
      "Epoch 20 - train_f1_at_03:0.70622  valid_f1_at_03:0.65873\n",
      "Epoch 20 - train_f1_at_05:0.54946  valid_f1_at_05:0.60189\n",
      "Starting 21 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [18:02<00:00,  1.46s/it, loss=0.00334]\n",
      "100%|| 93/93 [04:08<00:00,  2.67s/it, loss=0.00328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - avg_train_loss: 0.00334  avg_val_loss: 0.00328  time: 1333s\n",
      "Epoch 21 - train_f1_at_03:0.68883  valid_f1_at_03:0.70674\n",
      "Epoch 21 - train_f1_at_05:0.53706  valid_f1_at_05:0.65357\n",
      ">>>>>>>> Model Improved From 0.6867027782631487 ----> 0.7067395264116576\n",
      "other scores here... 0.7067395264116576, 0.6535714285714286\n",
      "Starting 22 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [17:58<00:00,  1.45s/it, loss=0.00316]\n",
      "100%|| 93/93 [04:02<00:00,  2.60s/it, loss=0.0035]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 - avg_train_loss: 0.00316  avg_val_loss: 0.00350  time: 1322s\n",
      "Epoch 22 - train_f1_at_03:0.70849  valid_f1_at_03:0.69446\n",
      "Epoch 22 - train_f1_at_05:0.56164  valid_f1_at_05:0.64667\n",
      "Starting 23 epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 743/743 [17:50<00:00,  1.44s/it, loss=0.00278]\n",
      "100%|| 93/93 [04:03<00:00,  2.62s/it, loss=0.00423]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23 - avg_train_loss: 0.00278  avg_val_loss: 0.00423  time: 1315s\n",
      "Epoch 23 - train_f1_at_03:0.74513  valid_f1_at_03:0.63954\n",
      "Epoch 23 - train_f1_at_05:0.61767  valid_f1_at_05:0.58766\n"
     ]
    }
   ],
   "source": [
    "# main loop\n",
    "for fold in range(5):\n",
    "    if fold not in CFG.folds:\n",
    "        continue\n",
    "    print(\"=\" * 100)\n",
    "    print(f\"Fold {fold} Training\")\n",
    "    print(\"=\" * 100)\n",
    "\n",
    "    trn_df = train[train.kfold != fold].reset_index(drop=True)\n",
    "    val_df = train[train.kfold == fold].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = WaveformDataset(df=trn_df, mode='train')\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=CFG.train_bs, num_workers=0, pin_memory=True, shuffle=True\n",
    "    )\n",
    "    \n",
    "    valid_dataset = WaveformDataset(df=val_df, mode='valid')\n",
    "    valid_dataloader = torch.utils.data.DataLoader(\n",
    "        valid_dataset, batch_size=CFG.valid_bs, num_workers=0, pin_memory=True, shuffle=False\n",
    "    )\n",
    "\n",
    "    model = TimmSED(\n",
    "        base_model_name=CFG.base_model_name,\n",
    "        pretrained=CFG.pretrained,\n",
    "        num_classes=CFG.num_classes,\n",
    "        in_channels=CFG.in_channels)\n",
    "\n",
    "    optimizer = transformers.AdamW(model.parameters(), lr=CFG.LR, weight_decay=CFG.WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=CFG.ETA_MIN, T_max=500)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    min_loss = 999\n",
    "    best_score = -np.inf\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        print(\"Starting {} epoch...\".format(epoch+1))\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        if epoch < CFG.cutmix_and_mixup_epochs:\n",
    "            train_avg, train_loss = train_mixup_cutmix_fn(model, train_dataloader, device, optimizer, scheduler)\n",
    "        else: \n",
    "            train_avg, train_loss = train_fn(model, train_dataloader, device, optimizer, scheduler)\n",
    "\n",
    "        valid_avg, valid_loss = valid_fn(model, valid_dataloader, device)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {train_loss:.5f}  avg_val_loss: {valid_loss:.5f}  time: {elapsed:.0f}s')\n",
    "        print(f\"Epoch {epoch+1} - train_f1_at_03:{train_avg['f1_at_03']:0.5f}  valid_f1_at_03:{valid_avg['f1_at_03']:0.5f}\")\n",
    "        print(f\"Epoch {epoch+1} - train_f1_at_05:{train_avg['f1_at_05']:0.5f}  valid_f1_at_05:{valid_avg['f1_at_05']:0.5f}\")\n",
    "\n",
    "        if valid_avg['f1_at_03'] > best_score:\n",
    "            print(f\">>>>>>>> Model Improved From {best_score} ----> {valid_avg['f1_at_03']}\")\n",
    "            print(f\"other scores here... {valid_avg['f1_at_03']}, {valid_avg['f1_at_05']}\")\n",
    "            torch.save(model.state_dict(), f'fold-{fold}.bin')\n",
    "            best_score = valid_avg['f1_at_03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fa78e4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-29T15:05:48.137151Z",
     "iopub.status.busy": "2022-03-29T15:05:48.136361Z",
     "iopub.status.idle": "2022-03-29T15:10:07.964192Z",
     "shell.execute_reply": "2022-03-29T15:10:07.957938Z"
    },
    "papermill": {
     "duration": 272.548754,
     "end_time": "2022-03-29T15:10:07.964356",
     "exception": false,
     "start_time": "2022-03-29T15:05:35.415602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 93/93 [04:10<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro f1_0.3 0.7067395264116576\n",
      "overall micro f1_0.3 0.7067395264116576\n",
      "overall micro f1_0.5 0.6535714285714286\n"
     ]
    }
   ],
   "source": [
    "model_paths = [f'fold-{i}.bin' for i in CFG.folds]\n",
    "\n",
    "calc_cv(model_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db99a10",
   "metadata": {
    "papermill": {
     "duration": 13.003501,
     "end_time": "2022-03-29T15:10:33.470059",
     "exception": false,
     "start_time": "2022-03-29T15:10:20.466558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30922.145559,
   "end_time": "2022-03-29T15:10:49.077990",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-03-29T06:35:26.932431",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
